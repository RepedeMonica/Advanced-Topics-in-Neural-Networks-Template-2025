seed: 42
device: "cuda"

dataset:
  name: "CIFAR100"   # CIFAR10 | CIFAR100 | MNIST | OXFORDIIITPET
  img_size: 32      # 28*28*60000 mnist | (32, 32): 50000 CIFAR10 & 100 | Min H: 108 Max H: 2606  Min W: 114 Max W: 3264 OXFORDIIITPET
  data_dir: "./data"
  cutmix_mixup: False
  tta: False

model:
  name: "resnet50"  # resnet18 | resnet50 | resnest14d | resnest26d | mlp
  pretrained: True
  mlp:
    input_dim: 3072 # 3*32*32=3072 cifar10/100 | 1*28*28=784 mnist | 3*224*224=150528 OXFORDIIITPET
    hidden: [512, 1024, 2048, 1024, 512]

training:
  epochs: 20
  batch_size: 32
  num_workers: 4

optimizer:
  name: "Muon"   # SGD | Adam | AdamW | Muon | SAM
  lr: 0.0005
  weight_decay: 0.05
  momentum: 0.9
  nesterov: True
  ######
  betas: [0.9, 0.999]
  eps: 1e-8
  #####
  warmup: False

scheduler:
  name: "ReduceLROnPlateau"   # StepLR | ReduceLROnPlateau
  step_size: 10
  gamma: 0.2
  last_epoch: -1
  ######
  factor: 0.1
  mode: "min"
  patience: 5
  threshold: 1e-3
  cooldown: 2
  min_lr: 1e-7

batch_scheduler:
  start_bs: 32
  max_bs: 64
  step_size: 20
  scale_factor: 2

early_stopping:
    patience: 15
    delta: 0.0005
    mode: "min"

tensorboard: True
wandb: True
tb_dir: "runs/exp-pretrained"
wandb_project: "training_pipeline_CIFAR-100-pretrained"