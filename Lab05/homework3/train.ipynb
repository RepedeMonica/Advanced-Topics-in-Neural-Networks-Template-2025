{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91602cf4-3a13-46f5-940c-c3aac2f861de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_config = False\n",
    "#config_path = \"config/config.yaml\"\n",
    "use_config = True\n",
    "config_path = \"config/config-nopretrain.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21edf6f5-051c-4536-90f6-929105f1cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "        \"method\": \"bayes\",\n",
    "        \"metric\": { \"name\": \"val/acc\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"model_name\": { \n",
    "                \"values\": [\"resnet50\", \"resnest26d\"]\n",
    "                #\"values\": [\"resnet18\", \"resnet50\", \"resnest14d\", \"resnest26d\"]\n",
    "            },\n",
    "            \"optimizer_name\": { \n",
    "                \"values\": [\"Muon\"]\n",
    "                #\"values\": [\"AdamW\", \"SGD\", \"Adam\", \"SAM\", \"Muon\"]\n",
    "            },\n",
    "            \"scheduler_name\": { \n",
    "                \"values\": [\"ReduceLROnPlateau\"]\n",
    "            },\n",
    "            \"lr_adam\": {\n",
    "                #\"values\": [2e-4, 3e-4, 5e-4, 1e-4]\n",
    "                    \"values\": [0.009]\n",
    "            },\n",
    "            \"weight_decay_adam\": {\n",
    "                #\"values\": [0.02, 0.05]\n",
    "                    \"values\": [5e-4]\n",
    "            },\n",
    "            \"lr_sam\": {\n",
    "                #\"values\": [0.05, 0.01, 0.02] #pretrain\n",
    "                \"values\": [0.05, 0.025]\n",
    "            },\n",
    "            \"lr_sgd\": {\n",
    "                #\"values\": [0.05, 0.01, 0.02] #pretrain\n",
    "                \"values\": [0.1, 0.05, 0.025]\n",
    "            },\n",
    "            \"weight_decay_sgd\": {\n",
    "                \"values\": [5e-4, 1e-4, 1e-3]\n",
    "            },\n",
    "            \"epochs\": {\n",
    "                \"value\": 200\n",
    "            },\n",
    "            \"rho\": {\n",
    "                \"values\": [0.05, 0.1]\n",
    "            },\n",
    "            \"gamma\": {\n",
    "                \"values\": [0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5022a593-8682-4287-9eaf-8dbae490500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from data.loader import get_dataloaders\n",
    "from models.model_factory import create_model\n",
    "from optim_scheduler.optim_factory import get_optimizer\n",
    "from optim_scheduler.scheduler_factory import get_scheduler\n",
    "from training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71af5cf-8462-4952-996e-4eca181b18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'tema3.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9942477-b8c0-4dab-bc09-06e3af6dc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb980fdb-a4e1-4f77-8fb2-c7fc7d37d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    return yaml.safe_load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e458bbe-f514-408b-9912-221afa3ef62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "def init_dist_if_needed():\n",
    "    if not dist.is_available():\n",
    "        return\n",
    "    if dist.is_initialized():\n",
    "        return\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\",                 \n",
    "        init_method=\"tcp://127.0.0.1:29500\",\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0ac04-c96d-432f-a0c5-c0945faa029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnest26d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find tema3.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmonicagabrielarepede\u001b[0m (\u001b[33mmonicagabrielarepede-universitatea-alexandru-ioan-cuza-d\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/root/data/test/homework3/wandb/run-20251123_180239-m3ots01y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain/runs/m3ots01y' target=\"_blank\">glowing-dragon-7</a></strong> to <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain/runs/m3ots01y' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/training_pipeline_CIFAR-100-no-pretrain/runs/m3ots01y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=4.3543 train_acc=1.3657 val_loss=3.9632 val_acc=0.1530 test_loss=4.0167 test_acc=0.1491 time=35.3s\n",
      "Epoch 2: train_loss=4.2230 train_acc=1.6662 val_loss=3.7950 val_acc=0.1992 test_loss=3.7809 test_acc=0.2002 time=36.6s\n",
      "Epoch 3: train_loss=4.1363 train_acc=1.8470 val_loss=3.6946 val_acc=0.2224 test_loss=3.7060 test_acc=0.2120 time=36.6s\n",
      "Epoch 4: train_loss=4.0450 train_acc=1.5730 val_loss=3.5571 val_acc=0.2654 test_loss=3.5593 test_acc=0.2578 time=34.8s\n"
     ]
    }
   ],
   "source": [
    "if use_config:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", default=config_path)\n",
    "    args = parser.parse_args(args=[])  \n",
    "    \n",
    "    cfg = load_config(args.config)\n",
    "    set_seed(cfg.get('seed', 42))\n",
    "    \n",
    "    device = torch.device(cfg.get('device','cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    print(cfg['model']['name'])\n",
    "    \n",
    "    is_muon = (cfg['optimizer']['name'] == \"Muon\")\n",
    "    is_sam = (cfg['optimizer']['name'] == \"SAM\")\n",
    "    \n",
    "    if is_muon:\n",
    "        init_dist_if_needed()\n",
    "    \n",
    "    train_loader, val_loader, test_loader, num_classes = get_dataloaders(cfg)\n",
    "    \n",
    "    model = create_model(cfg, num_classes)\n",
    "    model.to(device, non_blocking=True)\n",
    "    model = torch.jit.script(model)\n",
    "    \n",
    "    optim_obj = get_optimizer(cfg['optimizer']['name'], model, cfg)\n",
    "    scheduler = get_scheduler(cfg['scheduler']['name'], optim_obj, cfg) if cfg.get('scheduler') else None\n",
    "    \n",
    "    trainer = Trainer(model, optim_obj, scheduler, device, cfg, is_sam=is_sam)\n",
    "    trainer.fit(train_loader, val_loader, test_loader, cfg['training']['epochs']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c455a-d51e-4efb-b188-2d273965aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/KellerJordan/Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a700c7a-e719-4c42-944e-cd916ceb0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/davda54/sam.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3d19c-66d2-4adf-9115-0d33bb964a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def train_func(cfg_base):\n",
    "    with wandb.init(config=cfg_base):\n",
    "        cfg = wandb.config\n",
    "        set_seed(cfg.get('seed', 42))\n",
    "        \n",
    "        device = torch.device(cfg.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "        cfg['model']['name'] = cfg.model_name\n",
    "        cfg['optimizer']['name'] = cfg.optimizer_name\n",
    "        if (cfg['optimizer']['name']==\"AdamW\" or cfg['optimizer']['name']==\"Muon\"):\n",
    "            cfg['optimizer']['lr'] = float(cfg.lr_adam)\n",
    "        if (cfg['optimizer']['name']==\"SGD\"):\n",
    "            cfg['optimizer']['lr'] = float(cfg.lr_sgd)\n",
    "        if (cfg['optimizer']['name']==\"SAM\"):\n",
    "            cfg['optimizer']['lr'] = float(cfg.lr_sam)\n",
    "        \n",
    "        cfg['scheduler']['name'] = cfg.scheduler_name\n",
    "        cfg['training']['epochs'] = cfg.epochs\n",
    "        cfg[\"scheduler\"][\"gamma\"] = float(cfg.gamma)\n",
    "        \n",
    "        if (cfg['optimizer']['name']==\"AdamW\" or cfg['optimizer']['name']==\"Muon\"):\n",
    "            cfg['optimizer']['weight_decay'] = cfg.get(\"weight_decay_adam\", 0.01) \n",
    "        if (cfg['optimizer']['name']==\"SAM\" or cfg['optimizer']['name']==\"SGD\"):\n",
    "            cfg['optimizer']['weight_decay'] = cfg.get(\"weight_decay_sgd\", 0.01)\n",
    "\n",
    "        is_muon = (cfg['optimizer']['name'] == \"Muon\")\n",
    "        is_sam = (cfg['optimizer']['name'] == \"SAM\")\n",
    "\n",
    "        if is_sam:\n",
    "            cfg[\"optimizer\"][\"rho\"] = float(cfg.rho)\n",
    "        \n",
    "        if is_muon:\n",
    "            init_dist_if_needed() \n",
    "                \n",
    "        train_loader, val_loader, test_loader, num_classes = get_dataloaders(cfg) \n",
    "        model = create_model(cfg, num_classes)\n",
    "        \n",
    "\n",
    "        model.to(device, non_blocking=True)\n",
    "        model = torch.jit.script(model)\n",
    "        \n",
    "        optim_obj = get_optimizer(cfg['optimizer']['name'], model, cfg)\n",
    "        scheduler = get_scheduler(cfg['scheduler']['name'], optim_obj, cfg) if cfg.get('scheduler') else None\n",
    "        \n",
    "        trainer = Trainer(model, optim_obj, scheduler, device, cfg, is_sam=is_sam)\n",
    "        trainer.fit(train_loader, val_loader, test_loader, cfg['training']['epochs']) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784ea4-da96-4566-8bc9-19992292ebd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not use_config:\n",
    "    if __name__ == \"__main__\":\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--config\", default=config_path)\n",
    "        args = parser.parse_args(args=[])  \n",
    "        \n",
    "        cfg_base = load_config(args.config)\n",
    "        \n",
    "        sweep_id = wandb.sweep(sweep=sweep_config, project='cifar100-sweep3_nopretrain')\n",
    "        wandb.agent(sweep_id, function=lambda:train_func(cfg_base), count=70) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1505e-17c4-4dba-9e23-0642eb449e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
