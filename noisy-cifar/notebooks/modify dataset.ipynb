{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ec19a5-c7e9-4980-acec-2b281f11520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from typing import Optional, Callable\n",
    "import os\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2\n",
    "from torch.backends import cudnn\n",
    "from torch import GradScaler\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d5ffd-b68f-41cd-b55c-aec69712f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = [\n",
    "    'caformer_s18',\n",
    "    'caformer_m36',\n",
    "    'caformer_b36',\n",
    "    'maxvit_small_tf_224',\n",
    "    'maxvit_base_tf_224'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f634fa4-3c58-4213-92a5-dd15d3d3e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "cudnn.benchmark = True\n",
    "pin_memory = True\n",
    "enable_half = True  # Disable for CPU, it is slower!\n",
    "scaler = GradScaler(device, enabled=enable_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e603849b-0a9a-47c7-aefa-9f9ed3341af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCachedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.data = dataset.data\n",
    "        self.targets = dataset.targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.transform(self.data[i]), self.targets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21f39b2-9d76-47fd-a86f-d56d739a0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_noisy_fine(Dataset):\n",
    "    \"\"\"\n",
    "    See https://github.com/UCSC-REAL/cifar-10-100n, https://www.noisylabels.com/ and `Learning with Noisy Labels\n",
    "    Revisited: A Study Using Real-World Human Annotations`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root: str, train: bool, transform: Optional[Callable], download: bool\n",
    "    ):\n",
    "        cifar100 = CIFAR100(\n",
    "            root=root, train=train, transform=None, download=download\n",
    "        )\n",
    "        data, targets = tuple(zip(*cifar100))\n",
    "\n",
    "        if train:\n",
    "            noisy_label_file = os.path.join(root, \"CIFAR-100-noisy.npz\")\n",
    "            if not os.path.isfile(noisy_label_file):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"{type(self).__name__} need {noisy_label_file} to be used!\"\n",
    "                )\n",
    "\n",
    "            noise_file = np.load(noisy_label_file)\n",
    "            if not np.array_equal(noise_file[\"clean_label\"], targets):\n",
    "                raise RuntimeError(\"Clean labels do not match!\")\n",
    "            targets = noise_file[\"noisy_label\"]\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        return self.transform(self.data[i]), self.targets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f28b45b-db6f-46a9-8acf-08d7553531ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.RandAugment(num_ops=6, magnitude=9),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.5071, 0.4865, 0.4409), std=(0.2673, 0.2564, 0.2762), inplace=True),\n",
    "])\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.5071, 0.4865, 0.4409), std=(0.2673, 0.2564, 0.2762), inplace=True),\n",
    "])\n",
    "train_set = CIFAR100_noisy_fine('./fii-atnn-2024-project-noisy-cifar-100', download=False, train=True, transform=test_transform)\n",
    "test_set = CIFAR100_noisy_fine('./fii-atnn-2024-project-noisy-cifar-100', download=False, train=False, transform=test_transform)\n",
    "train_set = SimpleCachedDataset(train_set, transform=test_transform)\n",
    "test_set = SimpleCachedDataset(test_set, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_set, batch_size=8, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247d264d-8fe4-4f66-8463-872d53c0e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(name):\n",
    "    return timm.create_model(\n",
    "        name,\n",
    "        pretrained=True,\n",
    "        num_classes=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59447f82-a3fb-4cd4-82ec-ca3c19c0e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference_to_get_predicted_labels():\n",
    "    models = []\n",
    "    for name in list_name:\n",
    "        model = create_base_model(name)\n",
    "        model_path = f'./v3/best_{name}.pth'\n",
    "        state = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    all_preds = []\n",
    "    for inputs, _ in tqdm(train_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=enable_half):\n",
    "            logits = [m(inputs) for m in models]\n",
    "            summed = torch.stack(logits, dim=0).sum(dim=0)\n",
    "\n",
    "        preds = summed.argmax(dim=1)          # [B]\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c779045a-edc0-4b66-95ba-2f2195163ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50000/50000 [1:40:38<00:00,  8.28it/s]\n"
     ]
    }
   ],
   "source": [
    "labels_pred = inference_to_get_predicted_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc19763d-59d2-4a95-86a3-e5e33d54818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24bc0e2-6e5e-4622-81f4-a531c76d98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference_to_get_test_labels():\n",
    "    models = []\n",
    "    for name in list_name:\n",
    "        model = create_base_model(name)\n",
    "        model_path = f'./v3/best_{name}.pth'\n",
    "        state = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    all_preds = []\n",
    "    for inputs, _ in tqdm(test_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=enable_half):\n",
    "            logits = [m(inputs) for m in models]\n",
    "            summed = torch.stack(logits, dim=0).sum(dim=0)\n",
    "\n",
    "        preds = summed.argmax(dim=1)          # [B]\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac24cb3f-d604-4d36-b3db-c97dc9215553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1250/1250 [02:45<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=8, pin_memory=pin_memory)\n",
    "test_pred = inference_to_get_test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef408ff-b4ee-4acb-950f-e97ac1538864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ccca79-badf-4276-81e8-a53c9907b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9720e561-fb68-40c5-bbf9-aa393cc3534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=1, pin_memory=pin_memory)\n",
    "\n",
    "acc = 0\n",
    "cont = 0\n",
    "for _, targ in test_loader:\n",
    "    if targ == test_pred[cont]:\n",
    "        acc += 1\n",
    "    cont += 1\n",
    "print(acc/len(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db6f1aa-606f-4524-9229-035c7b9bfc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "cifar100 = CIFAR100(root='./fii-atnn-2024-project-noisy-cifar-100', train=True, transform=None, download=False)\n",
    "data, targets = tuple(zip(*cifar100))\n",
    "\n",
    "noisy_label_file = os.path.join('./fii-atnn-2024-project-noisy-cifar-100', \"CIFAR-100-noisy.npz\")\n",
    "if not os.path.isfile(noisy_label_file):\n",
    "    raise FileNotFoundError(f\"{type(self).__name__} need {noisy_label_file} to be used!\")\n",
    "\n",
    "noise_file = np.load(noisy_label_file)\n",
    "if not np.array_equal(noise_file[\"clean_label\"], targets):\n",
    "    raise RuntimeError(\"Clean labels do not match!\")\n",
    "targets = noise_file[\"noisy_label\"]\n",
    "\n",
    "print(len(data))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a417bd-f88f-409e-93a4-6cfd05746259",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create dataset 1: delete instances with pred != noisy\n",
    "### create dataset 2: change the label for instances with pred != noisy with pred value\n",
    "\n",
    "new_data_1 = []\n",
    "new_targets_1 = []\n",
    "new_data_2 = []\n",
    "new_targets_2 = []\n",
    "\n",
    "new_values_1 = [0] * 100\n",
    "new_values_2 = [0] * 100\n",
    "\n",
    "idx = 0\n",
    "for i, j in zip(labels_pred, targets):\n",
    "    if i == j:\n",
    "        # new_data_1.append(data[idx])\n",
    "        # new_data_2.append(data[idx])\n",
    "        new_targets_1.append((i, idx))\n",
    "        new_targets_2.append((i, idx))\n",
    "\n",
    "        new_values_1[i] += 1\n",
    "        new_values_2[i] += 1\n",
    "    else:\n",
    "        # new_data_2.append(data[idx])\n",
    "        new_targets_2.append((i, idx))\n",
    "        \n",
    "        new_values_2[i] += 1\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd22fe12-4dfe-4e6f-bc7e-82be06b96833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[469, 369, 435, 383, 40, 424, 309, 354, 420, 458, 269, 382, 255, 391, 382, 380, 339, 391, 285, 382, 523, 423, 418, 462, 375, 320, 306, 306, 373, 274, 314, 411, 145, 310, 329, 307, 406, 390, 274, 407, 351, 356, 232, 428, 355, 136, 618, 174, 448, 373, 387, 290, 428, 462, 325, 108, 318, 344, 488, 46, 284, 507, 391, 66, 160, 277, 270, 192, 412, 355, 340, 373, 135, 253, 93, 210, 344, 337, 333, 304, 254, 147, 495, 382, 346, 331, 462, 372, 370, 392, 369, 218, 241, 345, 423, 141, 107, 284, 465, 342]\n"
     ]
    }
   ],
   "source": [
    "print(new_values_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cfd07ad-82b5-45a2-95e4-65afe88cc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[543, 629, 543, 619, 155, 517, 469, 546, 495, 531, 362, 508, 387, 557, 494, 484, 458, 550, 460, 481, 585, 554, 491, 631, 479, 414, 536, 507, 522, 463, 674, 495, 276, 605, 566, 398, 782, 510, 464, 503, 466, 467, 414, 537, 538, 297, 751, 298, 500, 523, 1060, 498, 1103, 514, 509, 341, 562, 432, 555, 124, 436, 680, 706, 188, 322, 452, 567, 447, 561, 505, 450, 577, 381, 550, 343, 483, 508, 533, 453, 530, 489, 319, 556, 499, 458, 485, 515, 462, 533, 536, 516, 462, 374, 563, 522, 302, 292, 498, 612, 603]\n"
     ]
    }
   ],
   "source": [
    "print(new_values_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffea2c14-7aae-4e88-ae3e-3cde7ad1de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"dataset_delete_test_v4.npz\", targets=new_targets_1)\n",
    "np.savez(\"dataset_change_test_v4.npz\", targets=new_targets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca457065-2099-4eef-85fe-1b75dcbbea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "cifar100 = CIFAR100(root='./fii-atnn-2024-project-noisy-cifar-100', train=True, transform=None, download=False)\n",
    "data, targets = tuple(zip(*cifar100))\n",
    "\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687adcee-93ac-445c-9335-fd9d82d25ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8763455532308438 32979\n",
      "0.82916 50000\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for dataset in [new_targets_1, new_targets_2]:\n",
    "    acc = 0\n",
    "    for it in dataset:\n",
    "        if targets[it[1]] == it[0]:\n",
    "            acc += 1\n",
    "    print(acc / len(dataset), len(dataset))\n",
    "# for i, j in zip(targets, new_targets_2):\n",
    "#     if i == j[0]:\n",
    "#         acc += 1\n",
    "# print(acc / len(new_targets_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a4e04-a566-4b15-b5ad-07cc4ee0716f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4955abd6-f5b2-4583-8531-03bed2e70ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "cifar100 = CIFAR100(root='./fii-atnn-2024-project-noisy-cifar-100', train=True, transform=None, download=False)\n",
    "data, targets = tuple(zip(*cifar100))\n",
    "\n",
    "print(len(targets))\n",
    "\n",
    "noisy_label_file = os.path.join('./fii-atnn-2024-project-noisy-cifar-100', \"CIFAR-100-noisy.npz\")\n",
    "if not os.path.isfile(noisy_label_file):\n",
    "    raise FileNotFoundError(f\"{type(self).__name__} need {noisy_label_file} to be used!\")\n",
    "\n",
    "noise_file = np.load(noisy_label_file)\n",
    "if not np.array_equal(noise_file[\"clean_label\"], targets):\n",
    "    raise RuntimeError(\"Clean labels do not match!\")\n",
    "targets_noisy = noise_file[\"noisy_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33bdb7f7-488a-4109-90e0-62e61749bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for it in range(50000):\n",
    "    if targets[it] == targets_noisy[it]:\n",
    "        acc += 1\n",
    "print(acc / 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c173fa5-a181-4619-87d4-32f2409e14cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
