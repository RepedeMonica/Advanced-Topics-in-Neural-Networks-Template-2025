{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install timed-decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wandb tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import freeze_support\n",
    "from timed_decorator.simple_timed import timed\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10_images(data_path: str, train: bool):\n",
    "    initial_transforms = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "    cifar_10_images = CIFAR10(root=data_path, train=train, transform=initial_transforms, download=True)\n",
    "    return [image for image, label in cifar_10_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path: str = './data', train: bool = True, cache: bool = True):\n",
    "        self.images = get_cifar10_images(data_path, train)\n",
    "        self.cache = cache\n",
    "        self.transforms = v2.Compose([\n",
    "            v2.Resize((28, 28), antialias=True),\n",
    "            v2.Grayscale(),\n",
    "            v2.functional.hflip,\n",
    "            v2.functional.vflip,\n",
    "        ])\n",
    "        if cache:\n",
    "            self.labels = [self.transforms(x) for x in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.cache:\n",
    "            return self.images[i], self.labels[i]\n",
    "        return self.images[i], self.transforms(self.images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(3*32*32, 1*28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.reshape(x.size(0), 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "@timed(return_time=True, use_seconds=True, stdout=False)\n",
    "def transform_dataset_with_transforms(dataset: TensorDataset):\n",
    "    transforms = v2.Compose([\n",
    "        v2.Resize((28, 28), antialias=True),\n",
    "        v2.Grayscale(),\n",
    "        v2.functional.hflip,\n",
    "        v2.functional.vflip,\n",
    "    ])\n",
    "    for image in dataset.tensors[0]:\n",
    "        transforms(image)\n",
    "\n",
    "@timed(return_time=True, use_seconds=True, stdout=False)\n",
    "@torch.no_grad()\n",
    "def transform_dataset_with_model(dataset: TensorDataset, model: nn.Module, batch_size: int, device: torch.device):\n",
    "    model.eval()  \n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size,\n",
    "                           shuffle=False,\n",
    "                           num_workers=(4 if device.type==\"cuda\" else 0),\n",
    "                           pin_memory=(device.type==\"cuda\"))  # TODO: Complete the other parameters\n",
    "    for (images,) in dataloader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        model(images)  # TODO: uncomment this\n",
    "        #pass\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, train_loader, optim, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    device: torch.device,\n",
    "    data_path: str = \"./data\",\n",
    "    out_path: str = \"./weights.pt\",\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 1e-3,\n",
    "    max_epochs: int = 200,\n",
    "    patience: int = 10,\n",
    "    min_delta: float = 1e-4,\n",
    "):\n",
    "\n",
    "    wandb.init(project=\"CARN-optional\")\n",
    "    \n",
    "    full = CustomDataset(data_path=data_path, train=True, cache=True)\n",
    "    n = len(full)\n",
    "    n_val = max(5000, n // 10)\n",
    "    train_ds, val_ds = random_split(full, [n - n_val, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=(0 if device.type==\"cpu\" else 4), pin_memory=(device.type == \"cuda\"))\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=(0 if device.type==\"cpu\" else 4), pin_memory=(device.type == \"cuda\"))\n",
    "\n",
    "    model = Model().to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss = training_model(model, train_loader, optim, criterion, device)\n",
    "        val_loss = val_model(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train={train_loss:.6f} | val={val_loss:.6f}\")\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Validation Loss\": val_loss})\n",
    "        \n",
    "        if best_val - val_loss > min_delta:\n",
    "            best_val = val_loss\n",
    "            bad = 0\n",
    "            torch.save({\"model_state\": model.state_dict()}, out_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stopping after epoch {epoch}. Best val={best_val:.6f}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(out_path, map_location=device)[\"model_state\"])\n",
    "    wandb.finish()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_time(model: nn.Module, device=torch.device('cpu'), batch_size: int = 128):\n",
    "    test_dataset = CustomDataset(train=False, cache=False)\n",
    "    test_dataset = torch.stack(test_dataset.images)\n",
    "    test_dataset = TensorDataset(test_dataset)\n",
    "\n",
    "    #batch_size = 100  # TODO: add the other parameters (device, ...)\n",
    "\n",
    "    _, t1 = transform_dataset_with_transforms(test_dataset)\n",
    "\n",
    "    model = model.to(device)\n",
    "    _, t2 = transform_dataset_with_model(test_dataset, model, batch_size, device)\n",
    "    \n",
    "    print(f\"Sequential transforming each image took: {t1}s on CPU. \\n\"\n",
    "          f\"Using a model with batch_size: {batch_size} took {t2}s on {device.type}. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/root/data/test/opt/wandb/run-20251223_090832-gdxpcy2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional/runs/gdxpcy2f' target=\"_blank\">fresh-aardvark-8</a></strong> to <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional/runs/gdxpcy2f' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional/runs/gdxpcy2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train=0.043898 | val=0.007333\n",
      "Epoch 02 | train=0.005303 | val=0.003904\n",
      "Epoch 03 | train=0.003121 | val=0.002537\n",
      "Epoch 04 | train=0.002140 | val=0.001842\n",
      "Epoch 05 | train=0.001608 | val=0.001477\n",
      "Epoch 06 | train=0.001312 | val=0.001196\n",
      "Epoch 07 | train=0.001203 | val=0.001243\n",
      "Epoch 08 | train=0.001569 | val=0.001666\n",
      "Epoch 09 | train=0.001738 | val=0.003731\n",
      "Epoch 10 | train=0.001840 | val=0.001182\n",
      "Epoch 11 | train=0.002271 | val=0.001226\n",
      "Epoch 12 | train=0.002139 | val=0.003577\n",
      "Epoch 13 | train=0.001825 | val=0.001478\n",
      "Epoch 14 | train=0.001830 | val=0.004784\n",
      "Epoch 15 | train=0.001934 | val=0.000944\n",
      "Epoch 16 | train=0.002303 | val=0.001228\n",
      "Epoch 17 | train=0.001555 | val=0.000799\n",
      "Epoch 18 | train=0.002209 | val=0.000950\n",
      "Epoch 19 | train=0.001910 | val=0.002444\n",
      "Epoch 20 | train=0.001827 | val=0.001599\n",
      "Epoch 21 | train=0.001795 | val=0.001519\n",
      "Epoch 22 | train=0.002211 | val=0.000624\n",
      "Epoch 23 | train=0.002136 | val=0.001793\n",
      "Epoch 24 | train=0.001399 | val=0.000755\n",
      "Epoch 25 | train=0.001642 | val=0.001425\n",
      "Epoch 26 | train=0.002447 | val=0.002245\n",
      "Epoch 27 | train=0.001443 | val=0.003016\n",
      "Epoch 28 | train=0.001632 | val=0.004016\n",
      "Epoch 29 | train=0.002222 | val=0.000911\n",
      "Epoch 30 | train=0.001600 | val=0.001488\n",
      "Epoch 31 | train=0.002006 | val=0.002325\n",
      "Epoch 32 | train=0.001663 | val=0.002081\n",
      "Early stopping after epoch 32. Best val=0.000624\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▄▃▂▂▂▂▂▄▂▂▄▂▅▁▂▁▁▃▂▂▁▂▁▂▃▃▅▁▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>0.00166</td></tr><tr><td>Validation Loss</td><td>0.00208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-aardvark-8</strong> at: <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional/runs/gdxpcy2f' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional/runs/gdxpcy2f</a><br> View project at: <a href='https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional' target=\"_blank\">https://wandb.ai/monicagabrielarepede-universitatea-alexandru-ioan-cuza-d/CARN-optional</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251223_090832-gdxpcy2f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential transforming each image took: 1.116350301s on CPU. \n",
      "Using a model with batch_size: 128 took 0.29373099s on cuda. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = train_model(device=device)\n",
    "    test_inference_time(model, device, batch_size=128)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 comparisons to 'latex_images/'\n"
     ]
    }
   ],
   "source": [
    "def save_comparison_images(device, out_dir=\"latex_images\", num_images=5):\n",
    "    model = Model()\n",
    "    model.load_state_dict(torch.load(\"./weights.pt\", map_location=device)[\"model_state\"])\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    dataset = CustomDataset(train=False, cache=True)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            x, y_gt = dataset[i]          \n",
    "\n",
    "            x_in = x.unsqueeze(0)   \n",
    "            y_pred = model(x_in).squeeze(0)\n",
    "\n",
    "            save_image(x, f\"{out_dir}/input_{i}.png\")\n",
    "            save_image(y_gt, f\"{out_dir}/gt_{i}.png\")\n",
    "            save_image(y_pred, f\"{out_dir}/pred_{i}.png\")\n",
    "\n",
    "    print(f\"Saved {num_images} comparisons to '{out_dir}/'\")\n",
    "\n",
    "#save_comparison_images(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inference_time(model, device=torch.device(\"cuda\"), batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_times(batch_sizes=None, devices=None):\n",
    "    for dev in devices:\n",
    "        model = Model()\n",
    "        model.load_state_dict(torch.load(\"./weights.pt\", map_location=device)[\"model_state\"])\n",
    "        \n",
    "        for batch in batch_sizes:\n",
    "            test_inference_time(model, dev, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook carn_optional_v1.ipynb to script\n",
      "[NbConvertApp] Writing 7831 bytes to carn_optional_v1.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script carn_optional_v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
